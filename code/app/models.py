# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at

    https://colab.research.google.com/drive/1l-GhftAc88353ZxsgR3BRrTLhcpH8uKd

"""

import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")
from sklearn.linear_model import ElasticNet

"""## Load the cleaned dataset"""

train_data = pd.read_csv("./data/train_data.csv")
test_data = pd.read_csv("./data/test_data.csv")

train_data

test_data

housing_dataset = pd.read_csv("./data/output/output.csv")
housing_dataset

ntrain = train_data.shape[0]
ntest = test_data.shape[0]
copy_train = housing_dataset[:ntrain]
copy_test = housing_dataset[ntrain:]

copy_train

copy_test

#Saving the variable to predict in a different variable
Current_Saleprice = train_data['SalePrice']
Current_Saleprice

"""## Modeling"""

#We start the model analysis and compare the models 
#multi- linear regression
from sklearn import linear_model
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV,LogisticRegression
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

from sklearn.ensemble import GradientBoostingRegressor
from mlxtend.regressor import StackingCVRegressor
from sklearn.model_selection import KFold # for repeated K-fold cross validation
from sklearn.model_selection import cross_val_score # score evaluation
from sklearn.metrics import r2_score

#Calculate root mean squared error
#As the range of the values is going to be 0 to Current_Saleprice.max() we divide the rmse by Current_Saleprice.max() 
#so we get the rmse value on a range from 0 to 1 for better analysing purposes.
def root_mean_squared_error(y_pred):
    return np.sqrt(mean_squared_error(y_pred,Current_Saleprice))/Current_Saleprice.max()

# Return root mean square error applied cross validation (Used for training prediction)

#As the range of the values is going to be 0 to Current_Saleprice.max() we divide the cross-validation-score by Current_Saleprice.max() 
#so we get the cross-validation-score value on a range from 0 to 1 for better analysing purposes.

def cross_validation_score(model):
    return np.sqrt(-cross_val_score(model, copy_train, Current_Saleprice, scoring="neg_mean_squared_error", cv=kfolds)).mean()/Current_Saleprice.max()

#Set the number of folds
kfolds= KFold(n_splits = 10,shuffle=True)

"""### Linear regression

Linear regression is one of the most used regression algorithms so , it was a given to use this algorithm .

As there are multiple independent variables and just one target variable , we used multiple regression , where you can predict the value of the target variable using two or more independent variables.
"""

# Initialize linear regression instance
multiple_regression = linear_model.LinearRegression()
#Training the model using the training data
multiple_regression.fit(copy_train,Current_Saleprice)

#Predicting the model on the training data itself
multiple_regression_y_pred = multiple_regression.predict(copy_train)

#root mean square error of the model
multiple_regression_rmse = root_mean_squared_error(multiple_regression_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
multiple_regression_cross_val_score = cross_validation_score(multiple_regression)

multiple_regression_r2_score = r2_score(Current_Saleprice,multiple_regression_y_pred)

print("R2 score of Multiple Regression Model is : {0}".format(multiple_regression_r2_score))


print("Root mean squared error of multi linear regression model : {0}".format(multiple_regression_rmse))

print("Cross validation score of multi linear regression model : {0}".format(multiple_regression_cross_val_score))

"""### Gradient Boosting Algorithm"""

# Initialize Gradient Boosting Regression instance
gbr = GradientBoostingRegressor(n_estimators=1000)

#Training the model using the training data
gbr.fit(copy_train, Current_Saleprice)

#Predicting the model on the training data itself
gbr_y_pred = gbr.predict(copy_train)

#root mean square error of the model
gbr_rmse = root_mean_squared_error(gbr_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
gbr_cross_val_score = cross_validation_score(gbr)

gbr_r2_score = r2_score(Current_Saleprice,gbr_y_pred)

print("R2 score of Gradient Boosting Regression Model is : {0}".format(gbr_r2_score))



print("Root mean squared error of Gradient Boosting Regression model : {0}".format(gbr_rmse))

print("Cross validation score of Gradient Boosting Regression model : {0}".format(gbr_cross_val_score))

"""### XGB Regressor 

XGB is an efficient implementation of the gradient boosting algorithm.

It consists of a class of ensemble machine learning models used for regression purposes.
"""

# Initialize XGB Regressor instance
xgbr = XGBRegressor(verbosity=0,n_estimators=1000) 
#Training the model using the training data
xgbr.fit(copy_train, Current_Saleprice)

#Predicting the model on the training data itself
xgbr_y_pred = xgbr.predict(copy_train)

#root mean square error of the model
xgbr_rmse = root_mean_squared_error(xgbr_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
xgbr_cross_val_score = cross_validation_score(xgbr)

xgbr_r2_score = r2_score(Current_Saleprice,xgbr_y_pred)

print("R2 score of XGB Regression Model is : {0}".format(xgbr_r2_score))


print("Root mean squared error of XGB Regressor model : {0}".format(format(xgbr_rmse,'.5f')))

print("Cross validation score of XGB Regressor model : {0}".format(xgbr_cross_val_score))

"""### Lasso Regression"""

# Initialize lasso regression instance
#Training the model using the training data
lasso_regression = LassoCV().fit(copy_train, Current_Saleprice)

#Predicting the model on the training data itself
lasso_regression_y_pred = lasso_regression.predict(copy_train)

#root mean square error of the model
lasso_regression_rmse = root_mean_squared_error(lasso_regression_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
lasso_regression_cross_val_score = cross_validation_score(lasso_regression)

lasso_regression_r2_score  = r2_score(Current_Saleprice,lasso_regression_y_pred)

print("R2 score of Lasso Regression Model is : {0}".format(lasso_regression_r2_score))
print("Root mean squared error of Lasso regression model : {0}".format(lasso_regression_rmse))

print("Cross validation score of Lasso regression model : {0}".format(lasso_regression_cross_val_score))

"""## Ridge Regression"""

# Initialize lasso regression instance
#Training the model using the training data
ridge_regression = RidgeCV().fit(copy_train, Current_Saleprice)

#Predicting the model on the training data itself
ridge_regression_y_pred = ridge_regression.predict(copy_train)

#root mean square error of the model
ridge_regression_rmse = root_mean_squared_error(ridge_regression_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
ridge_regression_cross_val_score = cross_validation_score(ridge_regression)

ridge_regression_r2_score  = r2_score(Current_Saleprice,ridge_regression_y_pred)

print("R2 score of Ridge Regression Model is : {0}".format(ridge_regression_r2_score))


print("Root mean squared error of Ridge regression model : {0}".format(ridge_regression_rmse))

print("Cross validation score of Ridge regression model : {0}".format(ridge_regression_cross_val_score))

"""### Elastic Net Regression"""

#Training the model using the training data
elastic_net_regression = ElasticNet().fit(copy_train,Current_Saleprice)
#Predicting the model on the training data itself
elastic_net_regression_y_pred = elastic_net_regression.predict(copy_train)


elastic_net_regression_r2_score  = r2_score(Current_Saleprice,elastic_net_regression_y_pred)

print("R2 score of Elastic Net Regression Model is : {0}".format(elastic_net_regression_r2_score))

#root mean square error of the model
elastic_net_regression_rmse = root_mean_squared_error(elastic_net_regression_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
elastic_net_regression_cross_val_score = cross_validation_score(elastic_net_regression)


print("Root mean squared error of multi linear regression model : {0}".format(elastic_net_regression_rmse))

print("Cross validation score of multi linear regression model : {0}".format(elastic_net_regression_cross_val_score))

"""### Stacking Model"""

# Initialize Stacking model instance
stack_model = StackingCVRegressor(regressors=(multiple_regression, xgbr,elastic_net_regression,
                                              gbr,lasso_regression,ridge_regression),
                                  meta_regressor=xgbr, use_features_in_secondary=True)


#Training the model using the training data
stack_model.fit(copy_train, Current_Saleprice)

#Predicting the model on the training data itself
stack_model_y_pred = stack_model.predict(copy_train)

#root mean square error of the model
stack_model_rmse = root_mean_squared_error(stack_model_y_pred)

# Return root mean square error applied cross validation (Used for training prediction)
stack_model_cross_val_score = cross_validation_score(stack_model)

stack_model_r2_score  = r2_score(Current_Saleprice,stack_model_y_pred)

print("R2 score of Stacking Model is : {0}".format(stack_model_r2_score))


print("Root mean squared error of Stacking model : {0}".format(stack_model_rmse))

print("Cross validation score of Stacking model : {0}".format(stack_model_cross_val_score))

"""## Comparisons:"""

#Function to lower the numbers after decimal point
def lower_decimal(score_list):
    for i in range(len(score_list)):

        score_list[i] = float(format(score_list[i],'.4f'))
        

    return score_list

# A list of all the available models
models = ['Linear Regression','Gradient Boosting Regression',
          'XGBR Regressor','Lasso Regression','Ridge Regression','Elastic Regression','Stacking Model']

#Create a list of the rmse all the available models
models_rmse = [multiple_regression_rmse,
               gbr_rmse,
               xgbr_rmse,
               lasso_regression_rmse,
               ridge_regression_rmse,
               elastic_net_regression_rmse,
               stack_model_rmse]
models_rmse = lower_decimal(models_rmse)
models_rmse

#Create a list of the cross_val_score from all the available models

models_cross_val_score =[multiple_regression_cross_val_score,
                        gbr_cross_val_score,
                        xgbr_cross_val_score,
                         lasso_regression_cross_val_score,
                         ridge_regression_cross_val_score,
                         elastic_net_regression_cross_val_score,
                         stack_model_cross_val_score
                        ]
models_cross_val_score = lower_decimal(models_cross_val_score)
models_cross_val_score

#Create a list of the r2_score from all the available models

models_r2_score = [multiple_regression_r2_score,
                   gbr_r2_score,
                   xgbr_r2_score,
                   lasso_regression_r2_score,
                   ridge_regression_r2_score,
                    elastic_net_regression_r2_score,
                   stack_model_r2_score,
                  ]
models_r2_score = lower_decimal(models_r2_score)
models_r2_score

comparison_table = pd.DataFrame({'Models':models,
                               'rmse':models_rmse,
                               'cross_val_score':models_cross_val_score,
                                'r2_score':models_r2_score})
comparison_table = comparison_table.set_index(['Models'])
comparison_table

final_y_pred = stack_model.predict(copy_test)

output_df = pd.DataFrame({'Id': copy_test.index +1 ,
                   'SalePrice': final_y_pred})
output_df


# Comparison of the models 
# Define Data

model_length = models
rmse = models_rmse
cross_val_score = models_cross_val_score
r2_score = models_r2_score

x_axis = np.arange(len(model_length))

plt.figure(figsize=(22, 7))
# Multi bar Chart

plt.bar(x_axis , rmse, width=0.2, label = 'rmse')
plt.bar(x_axis +0.20, cross_val_score, width=0.2, label = 'cross_val_score')
plt.bar(x_axis +0.20*2, r2_score, width=0.2, label = 'r2_score')

# Xticks

plt.xticks(x_axis,model_length)

# Add legend
plt.title("Performance analysis of all regression models")

plt.legend()

# Display

plt.show()

output_df.to_csv('./data/output/submission.csv', index=False)




